from flask import Flask, render_template, request, jsonify
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import os
import subprocess
import torch
import librosa
import soundfile as sf
from datetime import datetime
from langdetect import detect_langs
from deep_translator import GoogleTranslator  
import asyncio

app = Flask(__name__)

# Configuration
ALLOWED_EXTENSIONS = {'wav', 'mp3', 'ogg', 'm4a', 'flac'}
UPLOAD_FOLDER = 'uploads'
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Hardware configuration
device = "cuda" if torch.cuda.is_available() else "cpu"

#  Load fine-tuned Whisper model
MODEL_PATH = r"F:\main_project\speech_to_text_webapp_deep_edit_04_main_project\speech_to_text_webapp_deep\whisper_finetuned_model-20250311T044337Z-001\whisper_finetuned_model"

try:
    processor = WhisperProcessor.from_pretrained(MODEL_PATH)
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)
    print(" Fine-tuned Whisper model loaded successfully!")
except Exception as e:
    print(f" Error loading fine-tuned model: {e}")
    processor, model = None, None  # Disable model processing if load fails


try:
    asyncio.set_event_loop(asyncio.new_event_loop())
except Exception as e:
    print(f"Error setting event loop: {e}")

# Function to check allowed file types
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

# Function to preprocess audio
def preprocess_audio(file_path):
    output_path = os.path.splitext(file_path)[0] + "_processed.wav"
    try:
        subprocess.run([
            'ffmpeg', '-i', file_path,
            '-ar', '16000',
            '-ac', '1',
            '-acodec', 'pcm_s16le',
            '-y',
            output_path
        ], check=True, stderr=subprocess.PIPE)
        return output_path
    except subprocess.CalledProcessError as e:
        print(f"Error in preprocess_audio: {e}")
        return None  # Return None if preprocessing fails

@app.route("/", methods=["GET", "POST"])
def index():
    if request.method == "POST":
        # Validate file
        if 'audio_file' not in request.files:
            return jsonify({"error": "No file uploaded"}), 400
        file = request.files['audio_file']
        if file.filename == '':
            return jsonify({"error": "No selected file"}), 400
        if not allowed_file(file.filename):
            return jsonify({"error": "Unsupported file format"}), 400

        try:
            # Secure filename and save
            timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
            filename = f"{timestamp}_{file.filename}"
            file_path = os.path.join(UPLOAD_FOLDER, filename)
            file.save(file_path)

            # Check file size
            if os.path.getsize(file_path) > MAX_FILE_SIZE:
                os.remove(file_path)
                return jsonify({"error": "File size exceeds 100MB limit"}), 400

            # Preprocess audio
            processed_path = preprocess_audio(file_path)
            if processed_path is None:
                os.remove(file_path)
                return jsonify({"error": "Audio preprocessing failed"}), 500

            #  Load audio correctly
            audio, sr = librosa.load(processed_path, sr=16000)
            sf.write(processed_path, audio, sr)

            #  Convert audio to input features
            if processor and model:  # Check if the model was loaded.
                input_features = processor(audio, sampling_rate=16000, return_tensors="pt").input_features.to(device)

                # Transcribe
                with torch.no_grad():
                    predicted_ids = model.generate(input_features)
                    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            else:
                os.remove(file_path)
                os.remove(processed_path)
                return jsonify({"error": "Whisper model not loaded."}), 500

            #  Language detection and translation
            try:
                detected_langs = detect_langs(transcription)
                detected_lang = detected_langs[0].lang if detected_langs else "unknown"

                if detected_lang != 'en' and transcription.strip():  # Translate if not English
                    translated_text = GoogleTranslator(source=detected_lang, target="en").translate(transcription)
                    result = {
                        "transcription": transcription,
                        "language": detected_lang,
                        "translation": translated_text
                    }
                else:  # If already English, return as is
                    result = {"transcription": transcription, "language": detected_lang}

            except Exception as e:
                result = {"transcription": transcription, "language": "Unknown", "error": f"Translation error: {str(e)}"}

            # Cleanup
            os.remove(file_path)
            os.remove(processed_path)

            return jsonify(result)

        except subprocess.CalledProcessError as e:
            return jsonify({"error": f"Audio processing failed: {e.stderr.decode()}"}), 500
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    return render_template('index.html')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
